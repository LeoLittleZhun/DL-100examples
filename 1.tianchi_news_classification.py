#!/usr/bin/env python
# -*- coding: utf-8 -*-
'''
@File    :   1.tianchi_news_classification.py
@Time    :   2024/02/25 21:46:01
@Author  :   ljc 
@Version :   1.0
@Desc    :   data from tianchi, news data classification
Validation Accuracy:  0.9227
'''
import torch
import pandas as pd
from transformers import AutoTokenizer, BertForSequenceClassification
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from sklearn.model_selection import train_test_split
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset
import os
from tqdm import tqdm
from sklearn.metrics import accuracy_score
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

# 1.ä½¿ç”¨mpsä½œä¸ºdevice
if torch.backends.mps.is_available():
    device = torch.device('mps')
else:
    device = torch.device('cpu')

### 2.è¯»å…¥æ•°æ®
# å¦‚æœä½¿ç”¨ç›¸å¯¹è·¯å¾„ä¸å¯è¡Œï¼Œéœ€è¦åœ¨vscode - è®¾ç½®ä¸­ - python â€º Terminal: Execute In File Dir æ‰“å¼€
# å¼•å…¥æ•°æ®é›†ï¼Œä»CSVæ–‡ä»¶ä¸­è¯»å…¥æ•°æ®
df = pd.read_csv('data/news_train_set.csv', encoding='utf-8',delimiter='\t')
# print(df.head(10))
# è§£æ text å­—æ®µï¼Œå°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•´æ•°åˆ—è¡¨
df["text"] = df["text"].apply(lambda x: [int(i) for i in x.split()])
# è®¾å®šæœ€å¤§é•¿åº¦(æ¯”å¦‚å–95%å¥å­çš„æœ€å¤§é•¿åº¦)
MAX_LEN = min(int(df["text"].apply(len).quantile(0.95)), 512)
print(f"MAX_LEN: {MAX_LEN}")
# è¿›è¡Œpaddingï¼Œä½¿æ‰€æœ‰çš„å¥å­é•¿åº¦ä¸€è‡´
sqs_i = 0
def pad_sequence(seq, max_len):
    global sqs_i
    sqs_i += 1
    if sqs_i % 10 == 0:
        print(f"å¤„ç†ç¬¬{sqs_i}ä¸ªå¥å­")
    # print(f"max_len: {max_len}")
    if len(seq) < max_len:
        seq += [0] * (max_len - len(seq)) # ç”¨0å¡«å……
        return seq
    else:
        return seq[:max_len] # è¶…è¿‡æœ€å¤§é•¿åº¦åˆ™æˆªæ–­
df["text"] = df["text"].apply(lambda x: pad_sequence(x, MAX_LEN)) # ä½¿ç”¨applyå‡½æ•°å¯¹æ¯ä¸€è¡Œè¿›è¡Œæ“ä½œ

# åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df["text"].tolist(), df["label"].tolist(), test_size=0.2, random_state=42)
print(f"train_texts: {len(train_texts)}, val_texts: {len(val_texts)}")
# 3.å¤„ç†tokenizer
# ä½¿ç”¨fast tokenizeråŠ é€Ÿ
tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese', use_fast=True)
print(f"tokenizer: {tokenizer}")
# åˆ†æ‰¹è¿›è¡Œtokenizer
def batch_tokenize(texts, batch_size=1000):
    encodings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        # ğŸš€ å…³é”®ä¿®æ”¹ï¼šæŠŠ List[int] è½¬æˆå­—ç¬¦ä¸²
        batch = [" ".join(map(str, seq)) for seq in batch]
        encoding = tokenizer(batch, padding="max_length", truncation=True, max_length=MAX_LEN, return_tensors="pt")
        encodings.append(encoding)
    return {
        "input_ids": torch.cat([e["input_ids"] for e in encodings]),
        "attention_mask": torch.cat([e["attention_mask"] for e in encodings])
    }
train_encodings = batch_tokenize(train_texts,batch_size=500)
val_encodings = batch_tokenize(val_texts,batch_size=500)
print(f"train_encodings, val_encodings å®Œæˆ")
# è½¬æ¢ä¸º Pytroch Tensor
train_labels = torch.tensor(train_labels, dtype=torch.long)
val_labels = torch.tensor(val_labels, dtype=torch.long)
print(f"pytorch tensor å®Œæˆ")
# 4.å®šä¹‰Datasetå’ŒDataLoader
class TextDataset(Dataset):
    def __init__(self, encodings, labels):
        self.input_ids = encodings['input_ids']
        self.attention_mask = encodings['attention_mask']
        self.labels = labels
    
    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return {
            "input_ids": self.input_ids[idx],
            "attention_mask": self.attention_mask[idx],
            "labels": self.labels[idx]
        }

# # åŠ è½½é¢„è®­ç»ƒçš„ BERT æ¨¡å‹å’Œåˆ†è¯å™¨
# tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
# print(f"tokenizer: {tokenizer}")
# Tokenize å¤„ç†
# def tokenize_function(text):
#     return tokenizer(
#         text,
#         padding='max_length',
#         truncation=True,
#         max_length=MAX_LEN,
#         # return_tensors='pt'
#     )
# å¯¹æ‰€æœ‰æ•°æ®è¿›è¡Œtokenization
# å…ˆçœ‹ä¸€ä¸‹train_textsçš„æ•°æ®ç±»å‹
# print(f"train_texts: {type(train_texts)},list_data:{type(train_texts[0])}, val_texts: {type(val_texts)}")
# train_texts = sum(train_texts, [])  # å°†äºŒç»´åˆ—è¡¨è½¬æ¢ä¸ºä¸€ç»´åˆ—è¡¨,ä¸Šä¸€æ­¥printçš„æ˜¯listç±»å‹ï¼Œéœ€è¦å±•å¼€
# val_texts = sum(val_texts, [])
# print(f"train_texts: {type(train_texts)}, val_texts: {type(val_texts)}")
# # train_encodings = tokenize_function(train_texts)
# train_encodings = tokenizer(train_texts, padding=True, truncation=True, max_length=MAX_LEN, return_tensors='pt')
# print(f"train_encodings")
# # val_encodings = tokenize_function(val_texts)
# val_encodings = tokenizer(val_texts, padding=True, truncation=True, max_length=MAX_LEN, return_tensors='pt')
# print(f"val_encodings")

BATCH_SIZE = 32
train_dataset = TextDataset(train_encodings, train_labels)
val_dataset = TextDataset(val_encodings, val_labels)
print('train_loader start')
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)

print(f"train_dataset: {len(train_dataset)}, val_dataset: {len(val_dataset)}, æ•°æ®è½¬æ¢å·²ç»å®Œæˆ")

# 5.å®šä¹‰æ¨¡å‹
# è®¡ç®—åˆ†ç±»æ•°é‡
num_labels = len(df["label"].unique())
model = BertForSequenceClassification.from_pretrained('bert-base-chinese', num_labels=num_labels)
model.to(device, dtype=torch.float32)
print(f"æ¨¡å‹å®šä¹‰å®Œæˆ")
optimizer = optim.AdamW(model.parameters(), lr=5e-5)
# å®šä¹‰æŸå¤±å‡½æ•°
loss_fn = nn.CrossEntropyLoss()
loss_fn.to(device, dtype=torch.float32)

num_epochs = 5 # è®­ç»ƒ5è½®
for epoch in range(num_epochs):
    model.train() # è®¾å®šä¸ºè®­ç»ƒæ¨¡å¼
    total_loss = 0

    loop = tqdm(train_loader, leave=True)
    for batch in loop:
        optimizer.zero_grad()

        # æ•°æ®ç§»åŠ¨åˆ°GPU
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        # å‰å‘ä¼ æ’­
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss

        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        # æ›´æ–°è¿›åº¦æ¡
        loop.set_description(f"Epoch {epoch+1}/{num_epochs}")
        loop.set_postfix(loss=total_loss / len(train_loader))
    print(f"Epoch {epoch+1}, Loss: {total_loss / len(train_loader)}")

# 7.éªŒè¯è¯„ä¼°æ¨¡å‹
model.eval() # è¯„ä¼°æ¨¡å¼
all_preds = []
all_labels = []

with torch.no_grad():
    for batch in val_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(input_ids, attention_mask=attention_mask)
        preds = outputs.logits.argmax(dim=1)

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

accuracy = accuracy_score(all_labels, all_preds)
print(f"Validation Accuracy: {accuracy: .4f}")

# ä¿å­˜æ¨¡å‹
if not os.path.exists('models'):
    os.makedirs('models')
model.save_pretrained('models/news_classification_model')
tokenizer.save_pretrained('models/news_classification_model')
print("æ¨¡å‹å·²ç»ä¿å­˜")
